# my approach 


first this is sentimental analysis 
text sentimental analysis is new to me rather than face sentimental analysis 

initital thought of me is to put all words in array and remove extras and punctuations and spaces and putting all unique set of words 
then remove the words that comes in both the positive cases adn negative cases 

but this problem is not that easy since in English sequence also matter to make sentence positive sentimental or negative sentimental 
so even if i do put all unique words in a list as positive or negative it would still get wrong result 

a very good example might be "not good" and "good" since not can be removed due to less uniqueness of the word and while good may signify that statement is good but its actually bad tho this issue can be solved using one thing that comes in my mind as per previous experience which is TF-IDR method Term frequency and Inverse Document Frequency which basically find the actual uniqueness , but more on this later but still keeping this in mind it might be useful in further methods 

so i must use a machine learning model which can analyse the sequence and pattern as well 

now the real problem is which one to pick and use 

as i have just use CNN convolutional neural network for my SIH project it was storing similar 128 dimensional features at nearby coordinates in that space to know the degree of similarity 
and as per my knowledge how LLMs work they also use the similar method of embedding similar words in a vector embedding to compare relativeness to each other 

here we can do it for 2 segments like positive or negative sentiment analysis , but idk how yet i still need to think and analyse further to see all options and the dataset 

it might just be a multi dimensional classification problem with use of vector embedding since the output is always either one or zero 1 or 0 so its not as complex as making an LLM thats a whole different case with whole different architecture which is transformer architecture but yes worth taking into context as this and that hold so much similarity to the approach like how i initially thought i should break it in array of all words same way like tokenisation but simpler in my case (much simpler)

i want to initially test it with something like classification model just like my previous project with logistic regression which is a classification model to classify 1 or 0 as the output but since like the last time i dont think text behaviour of English is so simple that we can solve it in a linear way or maybe we can taking TF-IDF and words of importance in axis of multiple dimensions and see the general trend of the areas that became either 0 or 1 according to classification then in a single sentence check for the ratio of how much % comes as 1 and as 0 then predict accordingly 
but i wanna take a non linear approach this time which should be better as above 






